{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b83fbd-3edb-43d6-b088-df5bba9c3c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/saiganesh/venv/lib/python3.12/site-packages (2.16.1)\n",
      "Requirement already satisfied: numpy in /home/saiganesh/venv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/saiganesh/venv/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in /home/saiganesh/venv/lib/python3.12/site-packages (4.41.2)\n",
      "Requirement already satisfied: scikit-learn in /home/saiganesh/venv/lib/python3.12/site-packages (1.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (70.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (4.12.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/saiganesh/venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/saiganesh/venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/saiganesh/venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in /home/saiganesh/venv/lib/python3.12/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/saiganesh/venv/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/saiganesh/venv/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/saiganesh/venv/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/saiganesh/venv/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/saiganesh/venv/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: rich in /home/saiganesh/venv/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /home/saiganesh/venv/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/saiganesh/venv/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/saiganesh/venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/saiganesh/venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/saiganesh/venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/saiganesh/venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/saiganesh/venv/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/saiganesh/venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/saiganesh/venv/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/saiganesh/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install neccessary libraries\n",
    "! pip install tensorflow numpy pandas transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16eca189-cb06-4756-9575-6fbaebb3a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 12:50:27.475566: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-31 12:50:27.475943: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-31 12:50:27.480109: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-31 12:50:27.512226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-31 12:50:28.499280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/saiganesh/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8425f5a6-757f-42af-a541-c6da1652aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the required file path\n",
    "df = pd.read_csv('/home/saiganesh/Desktop/classification_dataset.csv')\n",
    "\n",
    "# Define a function to update the category labels\n",
    "def update(cat):\n",
    "    \"\"\"\n",
    "    Update category labels.\n",
    "    \n",
    "    Args:\n",
    "    - cat (str): Category label to be updated.\n",
    "    \n",
    "    Returns:\n",
    "    - int: Updated category label (0 for \"Biology\", 1 for \"Finance\", unchanged otherwise).\n",
    "    \"\"\"\n",
    "    if cat == \"Biology\":\n",
    "        return 0\n",
    "    elif cat == \"Finance\":\n",
    "        return 1\n",
    "    elif cat == \"Java\":\n",
    "        return 2\n",
    "    return cat\n",
    "\n",
    "# Apply the update function to the 'Course' column in the DataFrame\n",
    "df.loc[:, \"Course\"] = df[\"Course\"].apply(update)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# Assign the 'Question' column to variable x and the updated 'Course' column to variable y\n",
    "x = df['Question']\n",
    "y = df['Course'].astype(np.int64)  # Update function is returning int, but TensorFlow v2 dataset accepts only int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab229e9c-353f-4ce0-98f5-33a252988e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name and maximum sequence length\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LEN = 20\n",
    "\n",
    "# x is a Pandas DataFrame containing Questions, converting it to a list\n",
    "review = x.to_list()\n",
    "\n",
    "# Initializing a DistilBERT tokenizer with the specified model name\n",
    "tkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenizing the Questions using the tokenizer, ensuring all inputs have the same length\n",
    "# max_length specifies the maximum length of each input sequence after tokenization\n",
    "# truncation=True ensures that sequences longer than max_length are truncated\n",
    "# padding=True ensures that sequences shorter than max_length are padded with the appropriate token\n",
    "inputs = tkzr(review, max_length=MAX_LEN, truncation=True, padding=True)\n",
    "\n",
    "# print(f'review: \\'{review}\\'')\n",
    "# print(f'input ids: {inputs[\"input_ids\"]}')\n",
    "# print(f'attention mask: {inputs[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f58c4f-7963-4cce-9331-bd378ae846f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):\n",
    "    \"\"\"\n",
    "    Construct token encodings for the input data using the provided tokenizer.\n",
    "\n",
    "    Args:\n",
    "    - x (list): List of input data (e.g., reviews).\n",
    "    - tkzr (PreTrainedTokenizer): Tokenizer object to tokenize the input data.\n",
    "    - max_len (int): Maximum length of each tokenized sequence.\n",
    "    - truncation (bool): Whether to truncate sequences longer than max_len (default=True).\n",
    "    - padding (bool): Whether to pad sequences shorter than max_len (default=True).\n",
    "\n",
    "    Returns:\n",
    "    - encodings (dict): Token encodings generated by the tokenizer.\n",
    "    \"\"\"\n",
    "    # Tokenize the input data using the provided tokenizer and specified parameters\n",
    "    encodings = tkzr(x, max_length=max_len, truncation=True, padding=True)\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "encodings = construct_encodings(x.to_list(), tkzr, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd2fda9-8118-4d5a-bb39-ba9ef76d0e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 12:52:16.780888: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "def construct_tfdataset(encodings, y=None):\n",
    "    \"\"\"\n",
    "    Construct a TensorFlow dataset from token encodings and labels (if provided).\n",
    "\n",
    "    Args:\n",
    "    - encodings (dict): Token encodings generated by a tokenizer.\n",
    "    - y (array-like, optional): Labels associated with the token encodings.\n",
    "\n",
    "    Returns:\n",
    "    - tfdataset (tf.data.Dataset): TensorFlow dataset containing token encodings and labels (if provided).\n",
    "    \"\"\"\n",
    "    if y is not None:\n",
    "        # If labels are provided, create a TensorFlow dataset with token encodings and labels\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings), y))\n",
    "    else:\n",
    "        # If labels are not provided (e.g., for inference/predictions), create a TensorFlow dataset with only token encodings\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "\n",
    "tfdataset = construct_tfdataset(encodings, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d86a2a0b-66cb-40ad-bf2b-67f060469207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test split ratio and batch size\n",
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Calculate the size of the training set based on the test split ratio\n",
    "train_size = int(len(x) * (1 - TEST_SPLIT))\n",
    "\n",
    "# Shuffle the dataset\n",
    "tfdataset = tfdataset.shuffle(len(x))\n",
    "\n",
    "# Split the shuffled dataset into training and testing sets\n",
    "tfdataset_train = tfdataset.take(train_size)\n",
    "tfdataset_test = tfdataset.skip(train_size)\n",
    "\n",
    "# Batch the training and testing datasets\n",
    "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n",
    "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d27d79f-3bd7-417f-acdf-de9dd9b6ed78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x7097c4e96cc0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Compile the model with the optimizer, loss function, and metrics\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(tfdataset_train, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, epochs\u001b[38;5;241m=\u001b[39mN_EPOCHS)\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:1563\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[0;32m-> 1563\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1575\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m   1576\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1583\u001b[0m     )\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/tf_keras/src/optimizers/__init__.py:335\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get(\n\u001b[1;32m    331\u001b[0m         config,\n\u001b[1;32m    332\u001b[0m         use_legacy_optimizer\u001b[38;5;241m=\u001b[39muse_legacy_optimizer,\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x7097c4e96cc0>"
     ]
    }
   ],
   "source": [
    "# Define the number of epochs\n",
    "N_EPOCHS = 2\n",
    "\n",
    "# Initialize the DistilBERT model for sequence classification\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optimizers.Adam(learning_rate=2e-5)\n",
    "loss_fn = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile the model with the optimizer, loss function, and metrics\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e78959-328e-432d-bf31-1624193e7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a163a-9ae7-496f-8e22-967600913370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the true labels from the test dataset\n",
    "y_test = []\n",
    "\n",
    "# Iterate through the test dataset to extract true labels\n",
    "for _, label in tfdataset_test.unbatch():\n",
    "    y_test.append(label.numpy())\n",
    "\n",
    "# Convert the list of true labels to a NumPy array\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Initialize an empty list to store the predicted labels\n",
    "y_pred = []\n",
    "\n",
    "# Iterate through batches of the test dataset to predict labels\n",
    "for batch in tfdataset_test:\n",
    "    # Extract inputs from the batch (excluding labels)\n",
    "    inputs, _ = batch\n",
    "    # Use the model to predict labels for the inputs\n",
    "    preds = model.predict(inputs).logits\n",
    "    # Extract the predicted labels and append to the list\n",
    "    y_pred.extend(np.argmax(preds, axis=-1))\n",
    "\n",
    "# Convert the list of predicted labels to a NumPy array\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Print true and predicted labels\n",
    "print(\"True labels (y_test):\", y_test)\n",
    "print(\"Predicted labels (y_pred):\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0fc82-1536-478c-b2d8-83a4a9864a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190507e-512d-4922-b0de-36c9fb3f48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictor(model, model_name, max_len):\n",
    "    \"\"\"\n",
    "    Create a predictor function for making predictions using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "    - model (TFDistilBertForSequenceClassification): Pre-trained model for sequence classification.\n",
    "    - model_name (str): Name of the pre-trained model.\n",
    "    - max_len (int): Maximum length of input sequences.\n",
    "\n",
    "    Returns:\n",
    "    - predict_proba (function): Predictor function that takes input text and returns the probability of the positive class i.e. label with 1.\n",
    "    \"\"\"\n",
    "    # Initialize a DistilBERT tokenizer with the specified model name\n",
    "    tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def predict_proba(text):\n",
    "        \"\"\"\n",
    "        Predict the probability of the positive class for the input text.\n",
    "\n",
    "        Args:\n",
    "        - text (str): Input text for prediction.\n",
    "\n",
    "        Returns:\n",
    "        - float: Probability of the positive class.\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        x = [text]\n",
    "        encodings = construct_encodings(x, tkzr, max_len=max_len)\n",
    "        tfdataset = construct_tfdataset(encodings)\n",
    "        tfdataset = tfdataset.batch(1)\n",
    "\n",
    "        # Make predictions using the pre-trained model\n",
    "        logits = model.predict(tfdataset).logits\n",
    "        probabilities = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "        \n",
    "        # Assuming binary classification, return probability of positive class (class 1)\n",
    "        # Adjust this according to your specific classification task\n",
    "        positive_probability = probabilities[:, 1]\n",
    "        return positive_probability[0]\n",
    "\n",
    "    return predict_proba\n",
    "\n",
    "# Create a predictor function using the trained model, tokenizer, and max_len\n",
    "clf = create_predictor(model, MODEL_NAME, MAX_LEN)\n",
    "\n",
    "# Test the predictor function with an example input\n",
    "print(clf('who is the father of biology?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de5f13-0a98-4a2e-a9ba-1f8e5933ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pre-trained model to the specified directory\n",
    "model.save_pretrained('./model/clf')\n",
    "\n",
    "# Save the model information (model name and maximum sequence length) to a pickle file\n",
    "import pickle\n",
    "with open('./model/info.pkl', 'wb') as f:\n",
    "    pickle.dump((MODEL_NAME, MAX_LEN), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b1765-9f83-43a5-b9a1-5e61d167c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model from the specified directory\n",
    "new_model = TFDistilBertForSequenceClassification.from_pretrained('./model/clf')\n",
    "\n",
    "# Load the model information (model name and maximum sequence length) from the pickle file\n",
    "model_name, max_len = pickle.load(open('./model/info.pkl', 'rb'))\n",
    "\n",
    "# Create a predictor function using the loaded model and information\n",
    "clf = create_predictor(new_model, model_name, max_len)\n",
    "\n",
    "# Test the predictor function with an example input\n",
    "print(clf('what are financial markets'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classify",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
